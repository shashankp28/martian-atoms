{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multi_imbalance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmulti_imbalance\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresampling\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmdo\u001b[39;00m \u001b[39mimport\u001b[39;00m MDO\n\u001b[0;32m     20\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'multi_imbalance'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from multi_imbalance.resampling.mdo import MDO\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>split</th>\n",
       "      <th>derivatized</th>\n",
       "      <th>features_path</th>\n",
       "      <th>features_md5_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0000</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_features/S0000.csv</td>\n",
       "      <td>52ec6d6f8372500ab4e069b5fbdae6f9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0001</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_features/S0001.csv</td>\n",
       "      <td>348f90baed8a8189bf0d4c7b9ed9f965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0002</td>\n",
       "      <td>train</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train_features/S0002.csv</td>\n",
       "      <td>4686ad9bc3716966f63b6ff83d1d8324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0003</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_features/S0003.csv</td>\n",
       "      <td>de6b53605c5887967dc3661a3a711c2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0004</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train_features/S0004.csv</td>\n",
       "      <td>fbfd90092d10d15a5d6919327ddde2ab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id  split  derivatized             features_path  \\\n",
       "0     S0000  train          NaN  train_features/S0000.csv   \n",
       "1     S0001  train          NaN  train_features/S0001.csv   \n",
       "2     S0002  train          1.0  train_features/S0002.csv   \n",
       "3     S0003  train          NaN  train_features/S0003.csv   \n",
       "4     S0004  train          NaN  train_features/S0004.csv   \n",
       "\n",
       "                  features_md5_hash  \n",
       "0  52ec6d6f8372500ab4e069b5fbdae6f9  \n",
       "1  348f90baed8a8189bf0d4c7b9ed9f965  \n",
       "2  4686ad9bc3716966f63b6ff83d1d8324  \n",
       "3  de6b53605c5887967dc3661a3a711c2b  \n",
       "4  fbfd90092d10d15a5d6919327ddde2ab  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv('metadata.csv')\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "all_ids = list(metadata.sample_id)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute maximum mass:  649.96228\n",
      "Absolute mean difference:  0.0009202114950436702\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def metadata_parser(metadata):\n",
    "    train_dict = {}\n",
    "    val_dict = {}\n",
    "    for row in metadata.iterrows():\n",
    "        if row[1].split == 'train':\n",
    "            train_dict[row[1].sample_id] = row[1].features_path\n",
    "        else:\n",
    "            val_dict[row[1].sample_id] = row[1].features_path\n",
    "    return train_dict, val_dict\n",
    "\n",
    "def train_label_parser(train_labels):\n",
    "    train_labels_dict = {}\n",
    "    for row in train_labels.iterrows():\n",
    "        row_list = list(row[1])\n",
    "        train_labels_dict[row_list[0]] = row_list[1:]\n",
    "    columns = list(train_labels.columns)\n",
    "    return train_labels_dict, columns[1:]\n",
    "\n",
    "def get_ids_per_class():\n",
    "    ids_per_class = {}\n",
    "    for class_name in labels:\n",
    "        index = labels.index(class_name)\n",
    "        ids = []\n",
    "        for key, value in train_labels_dict.items():\n",
    "            if value[index] == 1:\n",
    "                ids.append(key)\n",
    "        ids_per_class[class_name] = ids\n",
    "    return ids_per_class\n",
    "\n",
    "def find_absolute_maximum_mass():\n",
    "    print(\"Finding absolute maximum mass\")\n",
    "    pb = tqdm(total=len(all_ids))\n",
    "    absolute_max = 0\n",
    "    for key, value in all_paths.items():\n",
    "        pb.update(1)\n",
    "        data = pd.read_csv(value)\n",
    "        data.sort_values('mass')\n",
    "        max_mass = data['mass'].max()\n",
    "        absolute_max = max(absolute_max, max_mass)\n",
    "    return absolute_max\n",
    "\n",
    "def data_parser(id):\n",
    "    path = all_paths[id]\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.sort_values('mass')\n",
    "    labels = list(data.columns)\n",
    "    data_json = {\n",
    "        'time': np.array(data.time),\n",
    "        'mass': np.array(data.mass),\n",
    "        'intensity': np.array(data.intensity),\n",
    "    }\n",
    "    return data_json\n",
    "\n",
    "def descretize(data, mass_range, interval):\n",
    "    mass = data['mass'].copy()\n",
    "    intensity = data['intensity'].copy()\n",
    "    num_elements = int(mass_range / interval) + 1\n",
    "    intensity_desc = np.zeros(num_elements)\n",
    "    for i in range(num_elements):\n",
    "        mass_start = i * interval\n",
    "        mass_end = (i + 1) * interval\n",
    "        mask = (mass >= mass_start) & (mass < mass_end)\n",
    "        if np.any(mask):\n",
    "            intensity_desc[i] = np.max(intensity[mask])\n",
    "    return intensity_desc\n",
    "\n",
    "\n",
    "def extract_features(data, num_features, id):\n",
    "    mz_array = data['mass']\n",
    "    intensity_array = data['intensity']\n",
    "    target_label_array = np.array(train_labels_dict[id])\n",
    "    features = np.vstack((mz_array, intensity_array)).T\n",
    "    selector = SelectKBest(score_func=f_classif, k=num_features)\n",
    "    selected_features = selector.fit_transform(features, target_label_array)\n",
    "    return selected_features\n",
    "\n",
    "def intensity_vs_mass_plotter(sample_id, discretized=False, mass_range=650, interval=0.5):\n",
    "    data = data_parser(sample_id)\n",
    "    if discretized:\n",
    "        data['intensity'] = descretize(data, mass_range, interval)\n",
    "        data['mass'] = np.array([i*interval for i in range(len(data['intensity']))])\n",
    "    fig, axs = plt.subplots(figsize=(20, 10))\n",
    "    axs.set_xlim(0, 650)\n",
    "    axs.plot(data['mass'], data['intensity'])\n",
    "\n",
    "def abosulte_average_deviation():\n",
    "    deviations = 0\n",
    "    count = 0\n",
    "    for sample_id in tqdm(all_ids):\n",
    "        data = data_parser(sample_id)\n",
    "        mass = data['mass']\n",
    "        for i in range(len(mass) - 1):\n",
    "            deviations += np.abs(mass[i+1] - mass[i])\n",
    "            count += 1\n",
    "    return deviations, count\n",
    "\n",
    "def descritize_dataset(dataset_paths, max_mass, step, file_name):\n",
    "    with open(file_name, 'w+') as f:\n",
    "        columns = ['sample_id']\n",
    "        num_elements = int(max_mass / step) + 1\n",
    "        for i in range(num_elements):\n",
    "            columns.append(f'I{i}')\n",
    "        f.write(','.join(columns) + '\\n')\n",
    "        for key, value in tqdm(dataset_paths.items()):\n",
    "            data = data_parser(key)\n",
    "            intensity_desc = descretize(data, max_mass, step)\n",
    "            max_intensity = np.max(intensity_desc)\n",
    "            values = key+','+','.join([str(i/max_intensity) for i in intensity_desc]) + '\\n'\n",
    "            f.write(values)\n",
    "\n",
    "def perform_pca(train_path, val_path, num_features):\n",
    "    desired_num_features = num_features\n",
    "    pca = PCA(n_components=desired_num_features)\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    val_data = pd.read_csv(val_path)\n",
    "    train_features = train_data.iloc[:, 1:-1].values\n",
    "    val_features = val_data.iloc[:, 1:-1].values\n",
    "\n",
    "    pca.fit(train_features)\n",
    "\n",
    "    reduced_features_train = pd.DataFrame(pca.transform(train_features))\n",
    "    reduced_features_val = pd.DataFrame(pca.transform(val_features))\n",
    "    reduced_features_train.insert(0, 'sample_id', train_data['sample_id'])\n",
    "    reduced_features_val.insert(0, 'sample_id', val_data['sample_id'])\n",
    "    \n",
    "    reduced_features_train.to_csv(f'train_pca{num_features}.csv', index=False)\n",
    "    reduced_features_val.to_csv(f'val_pca{num_features}.csv', index=False)\n",
    "\n",
    "train_path, val_path = metadata_parser(metadata)\n",
    "train_labels_dict, labels = train_label_parser(train_labels)\n",
    "all_paths = {**train_path, **val_path}\n",
    "ids_per_class = get_ids_per_class()\n",
    "# absolute_maximum_mass = find_absolute_maximum_mass()   Already found\n",
    "# absolute_average_deviation, total_count = abosulte_average_deviation() Already Done\n",
    "absolute_maximum_mass = 649.96228 \n",
    "absolute_mean_diff = 544345.6024272853/591544015\n",
    "print(\"Absolute maximum mass: \", absolute_maximum_mass)\n",
    "print(\"Absolute mean difference: \", absolute_mean_diff)\n",
    "\n",
    "\n",
    "# print(\"Descretizing train dataset\")\n",
    "# descritize_dataset(train_path, 650, 0.1, \"train_descrete.csv\")\n",
    "# print(\"Descretizing test dataset\")\n",
    "# descritize_dataset(val_path, 650, 0.1, \"val_descrete.csv\")\n",
    "\n",
    "# Reducing number of features using PCA\n",
    "# perform_pca('train_descrete.csv', 'val_descrete.csv', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((809, 1301), (809, 9))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_descrete_data(path, labels_dict=None):\n",
    "    data = pd.read_csv(path)\n",
    "    sample_ids = list(data.sample_id)\n",
    "    data.drop('sample_id', axis=1, inplace=True)\n",
    "    X = np.array(data)\n",
    "    y = []\n",
    "    if labels_dict is not None:\n",
    "        for sample_id in sample_ids:\n",
    "            y.append(labels_dict[sample_id])\n",
    "    return X, np.array(y)\n",
    "\n",
    "X, y = return_descrete_data('train_descrete.csv', train_labels_dict)\n",
    "input_shape = X.shape[1]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old sizes: (566, 1301) (566, 9) (243, 1301) (243, 9)\n",
      "(283, 1301) (283, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "test_size=0 should be either positive and smaller than the number of samples 809 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 120\u001b[0m\n\u001b[0;32m    118\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m100\u001b[39m))\n\u001b[0;32m    119\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOld sizes:\u001b[39m\u001b[39m\"\u001b[39m, X_train\u001b[39m.\u001b[39mshape, y_train\u001b[39m.\u001b[39mshape, X_val\u001b[39m.\u001b[39mshape, y_val\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 120\u001b[0m X_train, y_train \u001b[39m=\u001b[39m minority_oversample_data(X_train, y_train)\n\u001b[0;32m    121\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNewSize: \u001b[39m\u001b[39m\"\u001b[39m, X_train\u001b[39m.\u001b[39mshape, y_train\u001b[39m.\u001b[39mshape, X_val\u001b[39m.\u001b[39mshape, y_val\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[139], line 114\u001b[0m, in \u001b[0;36mminority_oversample_data\u001b[1;34m(X_train, y_train, oversample_percentage)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mprint\u001b[39m(X_over\u001b[39m.\u001b[39mshape, y_over\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    113\u001b[0m new_X, new_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([X_train, X_over]), np\u001b[39m.\u001b[39mconcatenate([y_train, y_over])\n\u001b[1;32m--> 114\u001b[0m final_x, _, final_y, _ \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, random_state\u001b[39m=\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39m100\u001b[39;49m))\n\u001b[0;32m    115\u001b[0m \u001b[39mreturn\u001b[39;00m final_x, final_y\n",
      "File \u001b[1;32mc:\\Users\\Shashank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Shashank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2181\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2173\u001b[0m train_size_type \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(train_size)\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind\n\u001b[0;32m   2175\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2176\u001b[0m     test_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2177\u001b[0m     \u001b[39mand\u001b[39;00m (test_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n_samples \u001b[39mor\u001b[39;00m test_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m   2178\u001b[0m     \u001b[39mor\u001b[39;00m test_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2179\u001b[0m     \u001b[39mand\u001b[39;00m (test_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m test_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m   2180\u001b[0m ):\n\u001b[1;32m-> 2181\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2182\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtest_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2183\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2184\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(test_size, n_samples)\n\u001b[0;32m   2185\u001b[0m     )\n\u001b[0;32m   2187\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2188\u001b[0m     train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2189\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n_samples \u001b[39mor\u001b[39;00m train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m   2190\u001b[0m     \u001b[39mor\u001b[39;00m train_size_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2191\u001b[0m     \u001b[39mand\u001b[39;00m (train_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m train_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m   2192\u001b[0m ):\n\u001b[0;32m   2193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrain_size=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m should be either positive and smaller\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m than the number of samples \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m or a float in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2196\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(0, 1) range\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_size, n_samples)\n\u001b[0;32m   2197\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: test_size=0 should be either positive and smaller than the number of samples 809 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model(model, X_val, y_val, history=None):\n",
    "    # Plot the training and validation loss\n",
    "    if history is not None:\n",
    "        train_loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "        plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        train_acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "        plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    y_pred = model.predict(X_val, verbose=False).round()\n",
    "    class_rep = classification_report(y_val, y_pred)\n",
    "    confusion_matrix = multilabel_confusion_matrix(y_val, y_pred)\n",
    "    print(class_rep)\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    \n",
    "    eps = 1e-7\n",
    "    per_label_log_loss = -(y_val * np.log(y_pred+eps) + (1 - y_val) * np.log(1 - y_pred+eps))\n",
    "    aggregated_log_loss = np.mean(per_label_log_loss)\n",
    "    \n",
    "    # Random Loss\n",
    "    prob = y_val.sum()/(y_val.shape[0]*y_val.shape[1])\n",
    "    y_random = np.random.binomial(1, prob, y_val.shape)\n",
    "    per_label_log_loss_rand = -(y_val * np.log(y_random+eps) + (1 - y_val) * np.log(1 - y_random+eps))\n",
    "    aggregated_log_loss_rand = np.mean(per_label_log_loss_rand)\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    for i, matrix in enumerate(confusion_matrix):\n",
    "        sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])\n",
    "        axes[i].set_title(f'{labels[i]} Confusion Matrix')\n",
    "        axes[i].set_xlabel('Predicted Label')\n",
    "        axes[i].set_ylabel('True Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Log Loss: \", aggregated_log_loss)\n",
    "    print(\"Log Loss Random: \", aggregated_log_loss_rand)\n",
    "    print(\"Accuracy: \", ((y_pred == y_val).sum(axis=1) == 9).sum()/y_pred.shape[0], \"%\")\n",
    "    print(\"Datapoints: \", y_pred.shape[0])\n",
    "    print(\"--------------------------------------------\")\n",
    "    return aggregated_log_loss\n",
    "\n",
    "\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model):\n",
    "        super(CustomCallback, self).__init__()\n",
    "        self.model = model\n",
    "        self.min_score = 100\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(X_val, verbose=False).round()\n",
    "        eps = 1e-7\n",
    "        per_label_log_loss = -(y_val * np.log(y_pred+eps) + (1 - y_val) * np.log(1 - y_pred+eps))\n",
    "        aggregated_log_loss = np.mean(per_label_log_loss)\n",
    "        if aggregated_log_loss < self.min_score:\n",
    "            if os.path.exists('best_model.h5'):\n",
    "                os.remove('best_model.h5')\n",
    "            self.model.save('best_model.h5')\n",
    "            self.min_score = aggregated_log_loss\n",
    "\n",
    "def select_indices_with_priority(weights):\n",
    "    first_index = random.choices(range(9), weights=weights)[0]\n",
    "    while True:\n",
    "        second_index = random.choices(range(9), weights=weights)[0]\n",
    "        if first_index != second_index:\n",
    "            break\n",
    "    return first_index, second_index            \n",
    "\n",
    "def minority_oversample_data(X_train, y_train, oversample_percentage=50):\n",
    "    N = X_train.shape[0]\n",
    "    N_new = int(N * oversample_percentage / 100)\n",
    "    X_with_one_class = X_train[y_train.sum(axis=1)>=1]\n",
    "    y_with_one_class = y_train[y_train.sum(axis=1)>=1]\n",
    "    X_over, y_over = [], []\n",
    "    weights = 1./np.sum(y_train, axis=0)\n",
    "    while len(X_over) < N_new:\n",
    "        try:\n",
    "            f, s = select_indices_with_priority(weights)\n",
    "            x1 = X_with_one_class[y_with_one_class[:, f]==1][0]\n",
    "            y1 = y_with_one_class[y_with_one_class[:, f]==1][0]\n",
    "            x2 = X_with_one_class[y_with_one_class[:, s]==1][0]\n",
    "            y2 = y_with_one_class[y_with_one_class[:, s]==1][0]\n",
    "            x_new = (x1 + x2)\n",
    "            x_new = x_new / np.sum(x_new)\n",
    "            y_new = np.logical_or(y1, y2).astype(int)\n",
    "            X_over.append(x_new)\n",
    "            y_over.append(y_new)\n",
    "        except:\n",
    "            continue\n",
    "    X_over = np.array(X_over)\n",
    "    y_over = np.array(y_over)\n",
    "    print(X_over.shape, y_over.shape)\n",
    "    new_X, new_y = np.concatenate([X_train, X_over]), np.concatenate([y_train, y_over])\n",
    "    num_samples = x_train.shape[0]\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "    return new_X, new_y\n",
    "        \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=random.randint(0, 100))\n",
    "print(\"Old sizes:\", X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "X_train, y_train = minority_oversample_data(X_train, y_train)\n",
    "print(\"NewSize: \", X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_aggregated_log_loss(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    per_label_log_loss = -(y_true * tf.math.log(y_pred+epsilon) + (1 - y_true) * tf.math.log(1 - y_pred+epsilon))\n",
    "    aggregated_log_loss = tf.reduce_mean(per_label_log_loss)\n",
    "    return aggregated_log_loss\n",
    "\n",
    "def train_tf_model(X_train, y_train, X_val, y_val, input_shape):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, mode='auto', min_delta=0.001)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(640, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(tf.keras.layers.Dense(320, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(160, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(120, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(100, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(80, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(40, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(9, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=multilabel_aggregated_log_loss, metrics=['accuracy'])\n",
    "    checkpoint = CustomCallback(model)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=400, batch_size=32, verbose=True, callbacks=[checkpoint])\n",
    "    model_tf = tf.keras.models.load_model('best_model.h5', custom_objects={'multilabel_aggregated_log_loss': multilabel_aggregated_log_loss})\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old sizes: (566, 1301) (566, 9) (243, 1301) (243, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 9 and the array at index 1 has size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m100\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOld sizes:\u001b[39m\u001b[39m\"\u001b[39m, X_train\u001b[39m.\u001b[39mshape, y_train\u001b[39m.\u001b[39mshape, X_val\u001b[39m.\u001b[39mshape, y_val\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m X_train, y_train \u001b[39m=\u001b[39m oversample_data(X_train, y_train)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNewSize: \u001b[39m\u001b[39m\"\u001b[39m, X_train\u001b[39m.\u001b[39mshape, y_train\u001b[39m.\u001b[39mshape, X_val\u001b[39m.\u001b[39mshape, y_val\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m X_val, X_eval, y_val, y_eval \u001b[39m=\u001b[39m train_test_split(X_val, y_val, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state\u001b[39m=\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m100\u001b[39m))\n",
      "Cell \u001b[1;32mIn[100], line 106\u001b[0m, in \u001b[0;36moversample_data\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Append the oversampled data to the output arrays\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     X_train_resampled \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((X_train_resampled, X_resampled))\n\u001b[1;32m--> 106\u001b[0m     y_train_resampled \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvstack((y_train_resampled, y_resampled))\n\u001b[0;32m    108\u001b[0m \u001b[39m# Return the resampled data\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m X_train_resampled[:num_data_points], y_train_resampled[:num_data_points]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Shashank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 9 and the array at index 1 has size 1024"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=random.randint(0, 100))\n",
    "print(\"Old sizes:\", X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "X_train, y_train = oversample_data(X_train, y_train)\n",
    "print(\"NewSize: \", X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "X_val, X_eval, y_val, y_eval = train_test_split(X_val, y_val, test_size=0.5, random_state=random.randint(0, 100))\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "model_tf, history_tf = train_tf_model(X_train, y_train, X_val, y_val, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_tf, X_val, y_val, history_tf)\n",
    "evaluate_model(model_tf, X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "run = False\n",
    "if run:\n",
    "    val_data = pd.read_csv('val_descrete.csv')\n",
    "    val_data_samples = val_data.sample_id\n",
    "    val_data_values = val_data.drop(['sample_id'], axis=1).values\n",
    "    predictions = model_tf.predict(val_data_values).round()\n",
    "    submission = pd.DataFrame(predictions)\n",
    "    submission.insert(0, 'sample_id', val_data_samples)\n",
    "    new_column_names = {i: label for i, label in enumerate(labels)}\n",
    "    submission.rename(columns=new_column_names, inplace=True)\n",
    "    submission.to_csv('mystic_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
